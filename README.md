# gpu_nvlink

## NVLINK programming model

## ceed benchmarks

## notes

nvidia-smi top -m
gdrcopy
spack info /rkwciv5
ls /usr/local/software/spack/a100-hpl
cudamemcpy
module load gdrcopy
MPS
try with oneAPI  if not / try with SYCL
USM - shared between host and device


not between all devices
buffers and accessors
breaking previous model of scheduling of kernels.
nvidia-smi nvlink -s


https://stackoverflow.com/questions/53174224/nvlink-or-pcie-how-to-specify-the-interconnect
https://medium.com/gpgpu/multi-gpu-programming-6768eeb42e2c
